{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import fire_and_forget\n",
    "from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import  utils_perf\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "import redis\n",
    "from bloom_filter2 import BloomFilter\n",
    "import lzma\n",
    "from dask import dataframe as dd\n",
    "distributed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using distributed computation on a slurm cluster here. This is my particular config. You will need to alter this: https://distributed.dask.org/en/stable/\n",
    "if distributed == True:\n",
    "    NCORE = 4\n",
    "    print('deploying cluster')\n",
    "    cluster = SLURMCluster(\n",
    "        #change theses settings for your cluster\n",
    "        walltime='4:00:00',\n",
    "        n_workers = NCORE,\n",
    "        cores=NCORE,\n",
    "        processes = NCORE,\n",
    "        interface='ib0',\n",
    "        memory=\"120GB\",\n",
    "        env_extra=[\n",
    "\n",
    "        path + 'miniconda/etc/profile.d/conda.sh',\n",
    "        'conda activate ML2'\n",
    "        ],\n",
    "        #scheduler_options={'interface': 'ens2f0' },\n",
    "        #if gpu node\n",
    "        scheduler_options={'interface': 'ens3f0' },\n",
    "        #extra=[\"--lifetime\", \"3h55m\", \"--lifetime-stagger\", \"4m\"]\n",
    "    )\n",
    "    print(cluster.job_script())\n",
    "else:\n",
    "    cluster = LocalCluster()\n",
    "    client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed == True:\n",
    "    print(cluster)\n",
    "    cluster.scale(jobs = 100)\n",
    "    print(cluster.dashboard_link)\n",
    "    client = Client(cluster , timeout='450s' , set_as_default=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find which species each of the cogs has an interaction in\n",
    "\n",
    "#link_df = dd.read_csv('/scratch/dmoi/datasets/STRING/protein.physical.links.detailed.v11.5.txt', blocksize=100e6 , header = 0, sep = ' ')\n",
    "link_df = dd.read_csv(path + 'datasets/STRING/protein.links.full.v11.5.txt',  blocksize=75e6 , header = 0, sep = ' ')\n",
    "print(link_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute bloom filters for protein pairs\n",
    "#this is prob the part we're going to plug our function into\n",
    "@dask.delayed\n",
    "def mapcogs(df ):\n",
    "    #you need a redis server running on your cluster for this to work. change your ip, port and db number accordingly\n",
    "    rdb = redis.Redis(host='10.202.12.174', port=6379, db=0)\n",
    "    if type( df ) == tuple:\n",
    "        df = df[0]\n",
    "    protlist1 = list(df.protein1.map(lambda x:str(x).strip()))\n",
    "    protlist2 = list(df.protein2.map(lambda x:str(x).strip()))\n",
    "    protlist = list(set(protlist1+protlist2))\n",
    "    data = rdb.mget(protlist)\n",
    "    mapper = dict(zip(protlist, data) )\n",
    "    df['COG1'] = df.protein1.map(mapper)\n",
    "    df['COG2'] = df.protein2.map(mapper)\n",
    "    df = df.dropna()\n",
    "    df['COG1'] = df.COG1.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['COG2'] = df.COG2.map(lambda x:str(x).replace(\"b\",'').replace(\"'\",'').strip() )\n",
    "    df['species'] = df.protein1.map(lambda x:x.split('.')[0])\n",
    "    df['coglinks'] = df.COG1 + '_' + df.COG2 + '_' + df.species\n",
    "    ret = set(df.coglinks.unique())\n",
    "    return ret\n",
    "@dask.delayed\n",
    "def return_filter(coglinks, verbose = True):\n",
    "    if type( coglinks ) == tuple:\n",
    "        coglinks = coglinks[0]\n",
    "    b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "    for p in coglinks:\n",
    "        b.add( p )\n",
    "    retlen = len(coglinks)\n",
    "    return   b , retlen\n",
    "\n",
    "@dask.delayed\n",
    "def sumfilter(f1,f2, total ):\n",
    "    if type( f1 ) == tuple:\n",
    "        f1 = f1[0]\n",
    "    if type( f2 ) == tuple:\n",
    "        f2 = f2[0]\n",
    "    f3 = f1.__ior__(f2)\n",
    "    return f3 , total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treesum(totalfilter):\n",
    "    print(len(totalfilter))\n",
    "    while len(totalfilter)>1:\n",
    "        next_round= []\n",
    "        for i in range(0,len(totalfilter),2):\n",
    "            if i+1 < len(totalfilter):\n",
    "                next_round.append( sumfilter( totalfilter[i][0] , totalfilter[i+1][0] , totalfilter[i][1]+totalfilter[i+1][1]  ) )\n",
    "        if len(totalfilter) % 2 !=0:\n",
    "            next_round.append(totalfilter[-1])\n",
    "        totalfilter = next_round\n",
    "        print(len(totalfilter))\n",
    "    return totalfilter\n",
    "\n",
    "\n",
    "b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "partitions  = link_df.to_delayed()\n",
    "print('map cogs')\n",
    "res1 = [ mapcogs(p) for p in partitions ]\n",
    "print('done')\n",
    "print('make filters')\n",
    "res2 = [ return_filter(p) for p in res1 ]\n",
    "finals =[]\n",
    "for chunk in range(int(len(res2)/1024)+1):\n",
    "    print(chunk*1024)\n",
    "    res3 = res2[chunk*1024:(chunk+1)*1024]\n",
    "    res4 = treesum(res3)\n",
    "    res4 = dask.compute(res4)\n",
    "    print(res4)\n",
    "    finals.append(res4[0])\n",
    "\n",
    "with open('bloomfinal_big.pkl' , 'wb' ) as finalout:\n",
    "    finalout.write(pickle.dumps(finals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_filter == True:\n",
    "    with open('bloomfinal_big.pkl' , 'wb' ) as finalout:\n",
    "        finalout.write(pickle.dumps(finals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bloomfinal_big.pkl' , 'rb' ) as finalout:\n",
    "    resfinal = pickle.loads(finalout.read()) \n",
    "print(resfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see if our filter function works\n",
    "def check_filters(element,filters):\n",
    "    for f in filters:\n",
    "        if element in f[0][0]:\n",
    "            return True\n",
    "    return False\n",
    "import functools\n",
    "bfilter = functools.partial(check_filters , filters= resfinal)\n",
    "#should be in there\n",
    "print(bfilter('COG1756_COG0088_4113'))\n",
    "#should not be in there...\n",
    "print(bfilter('crap'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
