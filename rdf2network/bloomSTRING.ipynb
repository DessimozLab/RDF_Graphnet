{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f810a613-9331-47fb-b6a0-ecdd5c8eadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import fire_and_forget\n",
    "from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import  utils_perf\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "import redis\n",
    "from bloom_filter2 import BloomFilter\n",
    "import lzma\n",
    "from dask import dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945c62ec-637c-41f6-9a90-805577d12025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/work/FAC/FBM/DBC/cdessim2/default/dmoi/projects/RDF_Graphnet/rdf2network', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python310.zip', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/lib-dynload', '', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/lib/python3.10/site-packages', '../../..']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "#sys.path.append( '/home/cactuskid13/miniconda3/pkgs/')\n",
    "print(sys.path)\n",
    "import ete3\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import dask\n",
    "import warnings\n",
    "from dask import dataframe as dd\n",
    "import pickle\n",
    "path = '/work/FAC/FBM/DBC/cdessim2/default/dmoi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43d102a8-56c5-48d2-9f59-3cd502d79cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying cluster\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=4\n",
      "#SBATCH --mem=19G\n",
      "#SBATCH -t 4:00:00\n",
      "\n",
      "/work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/envs/ML2/bin/python -m distributed.cli.dask_worker tcp://10.203.101.82:42139 --nthreads 1 --nworkers 4 --memory-limit 4.66GiB --name dummy-name --nanny --death-timeout 60 --interface ib0\n",
      "\n",
      "SLURMCluster(73602189, 'tcp://10.203.101.82:42139', workers=0, threads=0, memory=0 B)\n",
      "http://10.203.101.82:8787/status\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import fire_and_forget\n",
    "from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import  utils_perf\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "import redis\n",
    "from bloom_filter2 import BloomFilter\n",
    "import lzma\n",
    "from dask import dataframe as dd\n",
    "\n",
    "#using distributed computation on a slurm cluster here. This is my particular config. You will need to alter this: https://distributed.dask.org/en/stable/\n",
    "NCORE = 4\n",
    "print('deploying cluster')\n",
    "cluster = SLURMCluster(\n",
    "    #change theses settings for your cluster\n",
    "    walltime='4:00:00',\n",
    "    n_workers = NCORE,\n",
    "    cores=NCORE,\n",
    "    processes = NCORE,\n",
    "    interface='ib0',\n",
    "    memory=\"20GB\",\n",
    "    #job_script_prologue=[\n",
    "    #' source /work/FAC/FBM/DBC/cdessim2/default/dmoi/miniconda3/etc/profile.d/conda.sh  ' ,\n",
    "    #'conda activate ML2'\n",
    "    #],\n",
    "    #scheduler_options={'interface': 'ens2f0' },\n",
    "    #if gpu node\n",
    "    scheduler_options={'interface': 'ens3f0np0' },\n",
    "    #extra=[ \"--lifetime-stagger\", \"4m\"]\n",
    ")\n",
    "print(cluster.job_script())\n",
    "print(cluster)\n",
    "cluster.scale(jobs = 100)\n",
    "print(cluster.dashboard_link)\n",
    "client = Client(cluster , timeout='450s' , set_as_default=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09c08bff-554b-4fa7-9bc9-53cfbc46c8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame Structure:\n",
      "                   protein1 protein2 neighborhood neighborhood_transferred fusion cooccurence homology coexpression coexpression_transferred experiments experiments_transferred database database_transferred textmining textmining_transferred combined_score\n",
      "npartitions=102701                                                                                                                                                                                                                                             \n",
      "                     object   object        int64                    int64  int64       int64    int64        int64                    int64       int64                   int64    int64                int64      int64                  int64          int64\n",
      "                        ...      ...          ...                      ...    ...         ...      ...          ...                      ...         ...                     ...      ...                  ...        ...                    ...            ...\n",
      "...                     ...      ...          ...                      ...    ...         ...      ...          ...                      ...         ...                     ...      ...                  ...        ...                    ...            ...\n",
      "                        ...      ...          ...                      ...    ...         ...      ...          ...                      ...         ...                     ...      ...                  ...        ...                    ...            ...\n",
      "                        ...      ...          ...                      ...    ...         ...      ...          ...                      ...         ...                     ...      ...                  ...        ...                    ...            ...\n",
      "Dask Name: read-csv, 102701 tasks\n"
     ]
    }
   ],
   "source": [
    "#find which species each of the cogs has an interaction in\n",
    "link_df = dd.read_csv(path + 'datasets/STRING/protein.links.full.v11.5.txt',  blocksize=15e6 , header = 0, sep = ' ')\n",
    "print(link_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d51e6-20b5-4567-8aee-227ba12b0798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map cogs\n",
      "done\n",
      "make filters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed: 124:  62%|██████▏   | 124/201 [3:42:45<2:15:15, 105.40s/it]"
     ]
    }
   ],
   "source": [
    "#compute bloom filters for protein pairs\n",
    "\n",
    "import tqdm \n",
    "\n",
    "@dask.delayed\n",
    "def protlinks_species( df ):\n",
    "    df = df[~df['protein1'].isna()]\n",
    "    df = df[~df['protein2'].isna()]\n",
    "    df.protein1 = df.protein1.map(lambda x:str(x))\n",
    "    df.protein2 = df.protein2.map(lambda x:str(x))\n",
    "    df['protlinks'] = df.protein1 + '_' + df.protein2 \n",
    "    ret = set(df.protlinks.unique())\n",
    "    return ret\n",
    "\n",
    "@dask.delayed\n",
    "def return_filter(protlinks, verbose = True):\n",
    "    if type( protlinks ) == tuple:\n",
    "        protlinks = protlinks[0]\n",
    "    b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "    for p in protlinks:\n",
    "        b.add( p )\n",
    "    retlen = len(protlinks)\n",
    "    return   b , retlen\n",
    "\n",
    "@dask.delayed\n",
    "def sumfilter(f1,f2, total ):\n",
    "    if type( f1 ) == tuple:\n",
    "        f1 = f1[0]\n",
    "    if type( f2 ) == tuple:\n",
    "        f2 = f2[0]\n",
    "    f3 = f1.__ior__(f2)\n",
    "    return f3 , total\n",
    "\n",
    "def treesum(totalfilter):\n",
    "    #print(len(totalfilter))\n",
    "    while len(totalfilter)>1:\n",
    "        next_round= []\n",
    "        for i in range(0,len(totalfilter),2):\n",
    "            if i+1 < len(totalfilter):\n",
    "                next_round.append( sumfilter( totalfilter[i][0] , totalfilter[i+1][0] , totalfilter[i][1]+totalfilter[i+1][1]  ) )\n",
    "        if len(totalfilter) % 2 !=0:\n",
    "            next_round.append(totalfilter[-1])\n",
    "        totalfilter = next_round\n",
    "        #print(len(totalfilter))\n",
    "    return totalfilter\n",
    "\n",
    "b=BloomFilter(max_elements=10**8, error_rate=0.001 ,start_fresh = True)\n",
    "partitions  = link_df.to_delayed()\n",
    "print('map cogs')\n",
    "res1 = [ protlinks_species(p) for p in partitions ]\n",
    "print('done')\n",
    "print('make filters')\n",
    "res2 = [ return_filter(p) for p in res1 ]\n",
    "\n",
    "\n",
    "finals =[]\n",
    "\n",
    "with tqdm.tqdm(total=int(len(res2)/512)+1) as pbar:\n",
    "    for chunk in range(int(len(res2)/512)+1):\n",
    "        #print(chunk*1024)\n",
    "        res3 = res2[chunk*512:(chunk+1)*512]\n",
    "        res4 = treesum(res3)\n",
    "        res4 = dask.compute(res4)\n",
    "        finals.append(res4[0])\n",
    "        pbar.set_description('processed: %d' % (1 + chunk ))\n",
    "        pbar.update(1)  \n",
    "\n",
    "#dask.compute(*finals)\n",
    "\n",
    "with open('bloomfinal_big.pkl' , 'wb' ) as finalout:\n",
    "    finalout.write(pickle.dumps(finals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06848a68-2f6f-495d-93b2-f4220d7d6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(link_df[].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0bcd9b-6a00-400d-8ccb-c827181eeb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "with open('bloomfinal_big.pkl' , 'rb' ) as finalout:\n",
    "    resfinal = pickle.loads(finalout.read()) \n",
    "print(resfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32adf1c-dc37-48aa-9040-5079948bad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1437758757 -100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934d2d4-e961-473b-ad87-6256998599f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_filters(element,filters):\n",
    "    for f in filters:\n",
    "        if element in f[0][0]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595747ef-2a88-4ec2-8fe9-caf2f16df8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check filters\n",
    "bfilter = functools.partial(check_filters , filters= resfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07877e3f-3d7a-4f97-82ff-a92afe788ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert True == bfilter('')\n",
    "assert True == bfilter('xxxxxxxxxxxxxxxxxxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2c61e-52eb-407e-ad7a-56566fd63286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
